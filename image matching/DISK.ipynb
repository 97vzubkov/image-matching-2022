{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DISK.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTQlX6QX5Au15REera+I/Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDg_6mht5iln","executionInfo":{"status":"ok","timestamp":1652270169366,"user_tz":-180,"elapsed":26785,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"d61b6ff1-43e3-47ad-f7c3-33b37de6b3c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["!mkdir -p /content/pip\n","!pip download --exists-action=w git+https://github.com/cvlab-epfl/disk@kaggle -d /content/pip/disk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctfJ_pw1562x","executionInfo":{"status":"ok","timestamp":1652270293495,"user_tz":-180,"elapsed":112799,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"521d133e-19d3-4e75-bf6d-d12115d5ddc2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/cvlab-epfl/disk@kaggle\n","  Cloning https://github.com/cvlab-epfl/disk (to revision kaggle) to /tmp/pip-req-build-0pv6m0ks\n","  Running command git clone -q https://github.com/cvlab-epfl/disk /tmp/pip-req-build-0pv6m0ks\n","  Running command git checkout -b kaggle --track origin/kaggle\n","  Switched to a new branch 'kaggle'\n","  Branch 'kaggle' set up to track remote branch 'kaggle' from 'origin'.\n","  Running command git submodule update --init --recursive -q\n","Collecting unets@ git+https://github.com/jatentaki/unets.git@no_norm_at_start\n","  Cloning https://github.com/jatentaki/unets.git (to revision no_norm_at_start) to /tmp/pip-download-3i65dy5s/unets_3870c9d371ce4c37bc7143fceb9d0916\n","  Running command git clone -q https://github.com/jatentaki/unets.git /tmp/pip-download-3i65dy5s/unets_3870c9d371ce4c37bc7143fceb9d0916\n","  Running command git checkout -b no_norm_at_start --track origin/no_norm_at_start\n","  Switched to a new branch 'no_norm_at_start'\n","  Branch 'no_norm_at_start' set up to track remote branch 'no_norm_at_start' from 'origin'.\n","Collecting torch-localize@ git+https://github.com/jatentaki/torch-localize.git@flatten\n","  Cloning https://github.com/jatentaki/torch-localize.git (to revision flatten) to /tmp/pip-download-3i65dy5s/torch-localize_6315d3242c92400080a76e9cbde7a922\n","  Running command git clone -q https://github.com/jatentaki/torch-localize.git /tmp/pip-download-3i65dy5s/torch-localize_6315d3242c92400080a76e9cbde7a922\n","  Running command git checkout -b flatten --track origin/flatten\n","  Switched to a new branch 'flatten'\n","  Branch 'flatten' set up to track remote branch 'flatten' from 'origin'.\n","Collecting torch-dimcheck@ git+https://github.com/jatentaki/torch-dimcheck.git\n","  Cloning https://github.com/jatentaki/torch-dimcheck.git to /tmp/pip-download-3i65dy5s/torch-dimcheck_49caec8d6f424cba98555ab8479bd3d0\n","  Running command git clone -q https://github.com/jatentaki/torch-dimcheck.git /tmp/pip-download-3i65dy5s/torch-dimcheck_49caec8d6f424cba98555ab8479bd3d0\n","Collecting torch\n","  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n","\u001b[K     |████████████████████████████████| 750.6 MB 13 kB/s \n","\u001b[?25hCollecting imageio\n","  Downloading imageio-2.19.1-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 57.1 MB/s \n","\u001b[?25hCollecting h5py\n","  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 58.2 MB/s \n","\u001b[?25hCollecting opencv-python\n","  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n","\u001b[K     |████████████████████████████████| 60.5 MB 1.2 MB/s \n","\u001b[?25hCollecting pydegensac\n","  Downloading pydegensac-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 65.4 MB/s \n","\u001b[?25hCollecting tensorboard\n","  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 44.8 MB/s \n","\u001b[?25hCollecting tensorflow\n","  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n","\u001b[K     |████████████████████████████████| 497.5 MB 32 kB/s \n","\u001b[?25hCollecting tqdm\n","  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 4.8 MB/s \n","\u001b[?25hCollecting numpy>=1.14.5\n","  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 62.7 MB/s \n","\u001b[?25hCollecting cached-property\n","  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Collecting pillow>=8.3.2\n","  Downloading Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 48.1 MB/s \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 6.4 MB/s \n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting absl-py>=0.4\n","  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 68.3 MB/s \n","\u001b[?25hCollecting requests<3,>=2.21.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n","\u001b[?25hCollecting setuptools>=41.0.0\n","  Downloading setuptools-62.2.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 57.4 MB/s \n","\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 57.2 MB/s \n","\u001b[?25hCollecting grpcio>=1.24.3\n","  Downloading grpcio-1.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 61.1 MB/s \n","\u001b[?25hCollecting werkzeug>=1.0.1\n","  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n","\u001b[K     |████████████████████████████████| 224 kB 61.9 MB/s \n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[K     |████████████████████████████████| 781 kB 90.2 MB/s \n","\u001b[?25hCollecting wheel>=0.26\n","  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Collecting google-auth<3,>=1.6.3\n","  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 86.4 MB/s \n","\u001b[?25hCollecting protobuf>=3.9.2\n","  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 56.8 MB/s \n","\u001b[?25hCollecting six\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting cachetools<6.0,>=2.0.0\n","  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n","Collecting rsa<5,>=3.1.4\n","  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","\u001b[K     |████████████████████████████████| 155 kB 65.6 MB/s \n","\u001b[?25hCollecting requests-oauthlib>=0.7.0\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4\n","  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n","Collecting typing-extensions>=3.6.4\n","  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n","Collecting pyasn1<0.5.0,>=0.4.6\n","  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 3.8 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 69.6 MB/s \n","\u001b[?25hCollecting charset-normalizer~=2.0.0\n","  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n","Collecting certifi>=2017.4.17\n","  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n","\u001b[K     |████████████████████████████████| 149 kB 70.2 MB/s \n","\u001b[?25hCollecting idna<4,>=2.5\n","  Downloading idna-3.3-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 5.2 MB/s \n","\u001b[?25hCollecting oauthlib>=3.0.0\n","  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 56.6 MB/s \n","\u001b[?25hCollecting google-pasta>=0.1.1\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 3.6 MB/s \n","\u001b[?25hCollecting opt-einsum>=2.3.2\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 2.7 MB/s \n","\u001b[?25hCollecting tensorboard\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 45.5 MB/s \n","\u001b[?25hCollecting gast>=0.2.1\n","  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n","Collecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 795 kB/s \n","\u001b[?25hCollecting flatbuffers>=1.12\n","  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n","Collecting astunparse>=1.6.0\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 55.8 MB/s \n","\u001b[?25hCollecting wrapt>=1.11.0\n","  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 2.3 MB/s \n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 69.8 MB/s \n","\u001b[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","Collecting libclang>=9.0.1\n","  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n","\u001b[K     |████████████████████████████████| 14.5 MB 48.9 MB/s \n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 51.3 MB/s \n","\u001b[?25hSaved ./pip/disk/disk-0.1.0.zip\n","Saved ./pip/disk/torch-dimcheck-0.0.1.zip\n","Saved ./pip/disk/torch-localize-0.1.0.zip\n","Saved ./pip/disk/unets-0.1.0.zip\n","Saved ./pip/disk/h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./pip/disk/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./pip/disk/cached_property-1.5.2-py2.py3-none-any.whl\n","Saved ./pip/disk/imageio-2.19.1-py3-none-any.whl\n","Saved ./pip/disk/Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./pip/disk/opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./pip/disk/pydegensac-0.1.2-cp37-cp37m-manylinux1_x86_64.whl\n","Saved ./pip/disk/absl_py-1.0.0-py3-none-any.whl\n","Saved ./pip/disk/google_auth-2.6.6-py2.py3-none-any.whl\n","Saved ./pip/disk/cachetools-5.0.0-py3-none-any.whl\n","Saved ./pip/disk/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl\n","Saved ./pip/disk/grpcio-1.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./pip/disk/Markdown-3.3.7-py3-none-any.whl\n","Saved ./pip/disk/importlib_metadata-4.11.3-py3-none-any.whl\n","Saved ./pip/disk/protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n","Saved ./pip/disk/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n","Saved ./pip/disk/pyasn1-0.4.8-py2.py3-none-any.whl\n","Saved ./pip/disk/requests-2.27.1-py2.py3-none-any.whl\n","Saved ./pip/disk/certifi-2021.10.8-py2.py3-none-any.whl\n","Saved ./pip/disk/charset_normalizer-2.0.12-py3-none-any.whl\n","Saved ./pip/disk/idna-3.3-py3-none-any.whl\n","Saved ./pip/disk/requests_oauthlib-1.3.1-py2.py3-none-any.whl\n","Saved ./pip/disk/oauthlib-3.2.0-py3-none-any.whl\n","Saved ./pip/disk/rsa-4.8-py3-none-any.whl\n","Saved ./pip/disk/six-1.16.0-py2.py3-none-any.whl\n","Saved ./pip/disk/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl\n","Saved ./pip/disk/tensorboard_plugin_wit-1.8.1-py3-none-any.whl\n","Saved ./pip/disk/typing_extensions-4.2.0-py3-none-any.whl\n","Saved ./pip/disk/urllib3-1.26.9-py2.py3-none-any.whl\n","Saved ./pip/disk/Werkzeug-2.1.2-py3-none-any.whl\n","Saved ./pip/disk/wheel-0.37.1-py2.py3-none-any.whl\n","Saved ./pip/disk/zipp-3.8.0-py3-none-any.whl\n","Saved ./pip/disk/tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl\n","Saved ./pip/disk/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\n","Saved ./pip/disk/astunparse-1.6.3-py2.py3-none-any.whl\n","Saved ./pip/disk/flatbuffers-2.0-py2.py3-none-any.whl\n","Saved ./pip/disk/gast-0.5.3-py3-none-any.whl\n","Saved ./pip/disk/google_pasta-0.2.0-py3-none-any.whl\n","Saved ./pip/disk/keras-2.8.0-py2.py3-none-any.whl\n","Saved ./pip/disk/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n","Saved ./pip/disk/libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl\n","Saved ./pip/disk/opt_einsum-3.3.0-py3-none-any.whl\n","Saved ./pip/disk/tensorboard-2.8.0-py3-none-any.whl\n","Saved ./pip/disk/tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./pip/disk/termcolor-1.1.0.tar.gz\n","Saved ./pip/disk/wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./pip/disk/torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl\n","Saved ./pip/disk/tqdm-4.64.0-py2.py3-none-any.whl\n","Saved ./pip/disk/setuptools-62.2.0-py3-none-any.whl\n","Successfully downloaded disk torch-dimcheck torch-localize unets h5py numpy cached-property imageio pillow opencv-python pydegensac absl-py google-auth cachetools google-auth-oauthlib grpcio markdown importlib-metadata protobuf pyasn1-modules pyasn1 requests certifi charset-normalizer idna requests-oauthlib oauthlib rsa six tensorboard-data-server tensorboard-plugin-wit typing-extensions urllib3 werkzeug wheel zipp tensorflow tf-estimator-nightly astunparse flatbuffers gast google-pasta keras keras-preprocessing libclang opt-einsum tensorboard tensorflow-io-gcs-filesystem termcolor wrapt torch tqdm setuptools\n"]}]},{"cell_type":"code","source":["!mkdir -p wheels\n","!pip wheel -w wheels git+https://github.com/jatentaki/unets.git@no_norm_at_start\n","!pip wheel -w wheels git+https://github.com/jatentaki/torch-localize.git@flatten\n","!pip wheel -w wheels git+https://github.com/jatentaki/torch-dimcheck\n","!pip wheel -w wheels git+https://github.com/etrulls/disk@kaggle\n","\n","!mkdir -p /content/pretrained\n","!wget https://github.com/etrulls/disk/raw/master/depth-save.pth -O /content/pretrained/disk-depth.pth\n","!wget https://github.com/etrulls/disk/raw/master/epipolar-save.pth -O /content/pretrained/disk-epipolar.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tibPrmf6ylU","executionInfo":{"status":"ok","timestamp":1652270336283,"user_tz":-180,"elapsed":42796,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"dee4864a-fb71-437d-dd07-900fb37d5b9e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/jatentaki/unets.git@no_norm_at_start\n","  Cloning https://github.com/jatentaki/unets.git (to revision no_norm_at_start) to /tmp/pip-req-build-yrkqk_9n\n","  Running command git clone -q https://github.com/jatentaki/unets.git /tmp/pip-req-build-yrkqk_9n\n","  Running command git checkout -b no_norm_at_start --track origin/no_norm_at_start\n","  Switched to a new branch 'no_norm_at_start'\n","  Branch 'no_norm_at_start' set up to track remote branch 'no_norm_at_start' from 'origin'.\n","Building wheels for collected packages: unets\n","  Building wheel for unets (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unets: filename=unets-0.1.0-py3-none-any.whl size=5183 sha256=a79b8eb3e9fb4a2f1e6d7b6f1b581e096702e2faf6820e56ddf6e91e98f77e58\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2fo9cz6h/wheels/e2/df/93/bf7aa9f3689d853c6b666c5d9ba423a5cffa5c08b1bb1c3710\n","Successfully built unets\n","Collecting git+https://github.com/jatentaki/torch-localize.git@flatten\n","  Cloning https://github.com/jatentaki/torch-localize.git (to revision flatten) to /tmp/pip-req-build-i71ujiz_\n","  Running command git clone -q https://github.com/jatentaki/torch-localize.git /tmp/pip-req-build-i71ujiz_\n","  Running command git checkout -b flatten --track origin/flatten\n","  Switched to a new branch 'flatten'\n","  Branch 'flatten' set up to track remote branch 'flatten' from 'origin'.\n","Building wheels for collected packages: torch-localize\n","  Building wheel for torch-localize (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-localize: filename=torch_localize-0.1.0-py3-none-any.whl size=2653 sha256=e6743aa5c51aec7fbb9c9e7e69420a12e4cfe588a33ccd2d8bebe98b4234cb97\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mz6i4kjd/wheels/bd/6e/3d/40c81ae42826d53d7914cbdcf798de337b40523fd3ea7eaad5\n","Successfully built torch-localize\n","Collecting git+https://github.com/jatentaki/torch-dimcheck\n","  Cloning https://github.com/jatentaki/torch-dimcheck to /tmp/pip-req-build-ankg014o\n","  Running command git clone -q https://github.com/jatentaki/torch-dimcheck /tmp/pip-req-build-ankg014o\n","Building wheels for collected packages: torch-dimcheck\n","  Building wheel for torch-dimcheck (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-dimcheck: filename=torch_dimcheck-0.0.1-py3-none-any.whl size=3460 sha256=76d53a403508873f558641ca022d7cc2fb0be56e2a6a2e93530d730bc48b1557\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7gjgkd8x/wheels/98/27/d7/fdc88be7b32e11d7f94a0f0a7fe6022d9a23be5afd89e9c770\n","Successfully built torch-dimcheck\n","Collecting git+https://github.com/etrulls/disk@kaggle\n","  Cloning https://github.com/etrulls/disk (to revision kaggle) to /tmp/pip-req-build-bh764ahq\n","  Running command git clone -q https://github.com/etrulls/disk /tmp/pip-req-build-bh764ahq\n","  Running command git checkout -b kaggle --track origin/kaggle\n","  Switched to a new branch 'kaggle'\n","  Branch 'kaggle' set up to track remote branch 'kaggle' from 'origin'.\n","  Running command git submodule update --init --recursive -q\n","Collecting torch\n","  Using cached torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n","Collecting imageio\n","  Using cached imageio-2.19.1-py3-none-any.whl (3.4 MB)\n","Collecting h5py\n","  Using cached h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n","Collecting opencv-python\n","  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n","Collecting pydegensac\n","  Using cached pydegensac-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.4 MB)\n","Collecting tensorboard\n","  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n","Collecting tensorflow\n","  Using cached tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n","Collecting tqdm\n","  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","Collecting numpy>=1.14.5\n","  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","Collecting cached-property\n","  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Collecting pillow>=8.3.2\n","  Using cached Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","Collecting absl-py>=0.4\n","  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n","Collecting google-auth<3,>=1.6.3\n","  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n","Collecting requests<3,>=2.21.0\n","  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","Collecting setuptools>=41.0.0\n","  Using cached setuptools-62.2.0-py3-none-any.whl (1.1 MB)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting wheel>=0.26\n","  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Collecting markdown>=2.6.8\n","  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0\n","  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","Collecting grpcio>=1.24.3\n","  Using cached grpcio-1.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","Collecting protobuf>=3.9.2\n","  Using cached protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","Collecting werkzeug>=1.0.1\n","  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n","Collecting six\n","  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting rsa<5,>=3.1.4\n","  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","Collecting cachetools<6.0,>=2.0.0\n","  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n","Collecting requests-oauthlib>=0.7.0\n","  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4\n","  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n","Collecting zipp>=0.5\n","  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n","Collecting typing-extensions>=3.6.4\n","  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n","Collecting pyasn1<0.5.0,>=0.4.6\n","  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","Collecting idna<4,>=2.5\n","  Using cached idna-3.3-py3-none-any.whl (61 kB)\n","Collecting certifi>=2017.4.17\n","  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n","Collecting charset-normalizer~=2.0.0\n","  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n","Collecting oauthlib>=3.0.0\n","  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n","Collecting tensorboard\n","  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","Collecting opt-einsum>=2.3.2\n","  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","Collecting flatbuffers>=1.12\n","  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n","Collecting google-pasta>=0.1.1\n","  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","Collecting keras-preprocessing>=1.1.1\n","  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Using cached tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n","Collecting keras<2.9,>=2.8.0rc0\n","  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","Collecting astunparse>=1.6.0\n","  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting gast>=0.2.1\n","  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n","Collecting termcolor>=1.1.0\n","  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n","Collecting wrapt>=1.11.0\n","  Using cached wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","Collecting libclang>=9.0.1\n","  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n","Saved ./wheels/h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./wheels/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./wheels/cached_property-1.5.2-py2.py3-none-any.whl\n","Saved ./wheels/imageio-2.19.1-py3-none-any.whl\n","Saved ./wheels/Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./wheels/opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./wheels/pydegensac-0.1.2-cp37-cp37m-manylinux1_x86_64.whl\n","Saved ./wheels/absl_py-1.0.0-py3-none-any.whl\n","Saved ./wheels/google_auth-2.6.6-py2.py3-none-any.whl\n","Saved ./wheels/cachetools-5.0.0-py3-none-any.whl\n","Saved ./wheels/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl\n","Saved ./wheels/grpcio-1.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./wheels/Markdown-3.3.7-py3-none-any.whl\n","Saved ./wheels/importlib_metadata-4.11.3-py3-none-any.whl\n","Saved ./wheels/protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n","Saved ./wheels/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n","Saved ./wheels/pyasn1-0.4.8-py2.py3-none-any.whl\n","Saved ./wheels/requests-2.27.1-py2.py3-none-any.whl\n","Saved ./wheels/certifi-2021.10.8-py2.py3-none-any.whl\n","Saved ./wheels/charset_normalizer-2.0.12-py3-none-any.whl\n","Saved ./wheels/idna-3.3-py3-none-any.whl\n","Saved ./wheels/requests_oauthlib-1.3.1-py2.py3-none-any.whl\n","Saved ./wheels/oauthlib-3.2.0-py3-none-any.whl\n","Saved ./wheels/rsa-4.8-py3-none-any.whl\n","Saved ./wheels/six-1.16.0-py2.py3-none-any.whl\n","Saved ./wheels/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl\n","Saved ./wheels/tensorboard_plugin_wit-1.8.1-py3-none-any.whl\n","Saved ./wheels/typing_extensions-4.2.0-py3-none-any.whl\n","Saved ./wheels/urllib3-1.26.9-py2.py3-none-any.whl\n","Saved ./wheels/Werkzeug-2.1.2-py3-none-any.whl\n","Saved ./wheels/wheel-0.37.1-py2.py3-none-any.whl\n","Saved ./wheels/zipp-3.8.0-py3-none-any.whl\n","Saved ./wheels/tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl\n","Saved ./wheels/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\n","Saved ./wheels/astunparse-1.6.3-py2.py3-none-any.whl\n","Saved ./wheels/flatbuffers-2.0-py2.py3-none-any.whl\n","Saved ./wheels/gast-0.5.3-py3-none-any.whl\n","Saved ./wheels/google_pasta-0.2.0-py3-none-any.whl\n","Saved ./wheels/keras-2.8.0-py2.py3-none-any.whl\n","Saved ./wheels/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n","Saved ./wheels/libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl\n","Saved ./wheels/opt_einsum-3.3.0-py3-none-any.whl\n","Saved ./wheels/tensorboard-2.8.0-py3-none-any.whl\n","Saved ./wheels/tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","Saved ./wheels/wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Saved ./wheels/torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl\n","Saved ./wheels/tqdm-4.64.0-py2.py3-none-any.whl\n","Saved ./wheels/setuptools-62.2.0-py3-none-any.whl\n","Building wheels for collected packages: disk, termcolor\n","  Building wheel for disk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for disk: filename=disk-0.1.0-py3-none-any.whl size=38078 sha256=7c5427e0c51ba6f135b9d58f6d3856dca76460157b8349079e550c2f506366be\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8ge2attq/wheels/27/85/9d/31bf987a13211ac84d8e88e4599254f0c4b1c296c709a74ffc\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=a5b6f48a4f3fbd99ca28486d3565c09a0f0face3175e61a7f93d0f5d999fe5f0\n","  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n","Successfully built disk termcolor\n","--2022-05-11 11:58:52--  https://github.com/etrulls/disk/raw/master/depth-save.pth\n","Resolving github.com (github.com)... 140.82.121.4\n","Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/etrulls/disk/master/depth-save.pth [following]\n","--2022-05-11 11:58:53--  https://raw.githubusercontent.com/etrulls/disk/master/depth-save.pth\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4375832 (4.2M) [application/octet-stream]\n","Saving to: ‘/content/pretrained/disk-depth.pth’\n","\n","/content/pretrained 100%[===================>]   4.17M  --.-KB/s    in 0.03s   \n","\n","2022-05-11 11:58:53 (121 MB/s) - ‘/content/pretrained/disk-depth.pth’ saved [4375832/4375832]\n","\n","--2022-05-11 11:58:54--  https://github.com/etrulls/disk/raw/master/epipolar-save.pth\n","Resolving github.com (github.com)... 140.82.121.3\n","Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/etrulls/disk/master/epipolar-save.pth [following]\n","--2022-05-11 11:58:54--  https://raw.githubusercontent.com/etrulls/disk/master/epipolar-save.pth\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4375832 (4.2M) [application/octet-stream]\n","Saving to: ‘/content/pretrained/disk-epipolar.pth’\n","\n","/content/pretrained 100%[===================>]   4.17M  --.-KB/s    in 0.04s   \n","\n","2022-05-11 11:58:55 (118 MB/s) - ‘/content/pretrained/disk-epipolar.pth’ saved [4375832/4375832]\n","\n"]}]},{"cell_type":"code","source":["!pip install -f /content/wheels --no-index torch_dimcheck\n","!pip install -f /content/wheels --no-index torch_localize\n","!pip install -f /content/wheels --no-index unets\n","!pip install -f /content/wheels --no-index disk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XM1mQR37v4D","executionInfo":{"status":"ok","timestamp":1652270352590,"user_tz":-180,"elapsed":16314,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"f3414a81-0f7c-48a6-896b-3b2eecedde12"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: /content/wheels\n","Processing ./wheels/torch_dimcheck-0.0.1-py3-none-any.whl\n","Installing collected packages: torch-dimcheck\n","Successfully installed torch-dimcheck-0.0.1\n","Looking in links: /content/wheels\n","Processing ./wheels/torch_localize-0.1.0-py3-none-any.whl\n","Installing collected packages: torch-localize\n","Successfully installed torch-localize-0.1.0\n","Looking in links: /content/wheels\n","Processing ./wheels/unets-0.1.0-py3-none-any.whl\n","Installing collected packages: unets\n","Successfully installed unets-0.1.0\n","Looking in links: /content/wheels\n","Processing ./wheels/disk-0.1.0-py3-none-any.whl\n","Processing ./wheels/pydegensac-0.1.2-cp37-cp37m-manylinux1_x86_64.whl\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from disk) (2.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from disk) (4.64.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from disk) (1.11.0+cu113)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from disk) (3.1.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from disk) (4.1.2.30)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from disk) (2.8.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from disk) (2.4.1)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py->disk) (1.21.6)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->disk) (1.5.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->disk) (7.1.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (3.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (2.23.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (1.44.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (0.6.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (0.37.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (1.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->disk) (0.4.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard->disk) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->disk) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->disk) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->disk) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->disk) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->disk) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->disk) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->disk) (4.2.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->disk) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->disk) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->disk) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->disk) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->disk) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->disk) (3.2.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (0.25.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (1.14.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (1.1.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (14.0.1)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (2.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (1.6.3)\n","Processing ./wheels/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->disk) (0.2.0)\n","Installing collected packages: tf-estimator-nightly, pydegensac, disk\n","Successfully installed disk-0.1.0 pydegensac-0.1.2 tf-estimator-nightly-2.8.0.dev2021122109\n"]}]},{"cell_type":"code","source":["!pip install opencv-python==4.5.5.62\n","!pip install opencv-contrib-python==4.5.5.62"],"metadata":{"id":"nm8Az60dVkQQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652270376028,"user_tz":-180,"elapsed":23443,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"0fd9db49-0cd9-4156-c888-cdea8d99e364"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-python==4.5.5.62\n","  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n","\u001b[K     |████████████████████████████████| 60.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.5.5.62) (1.21.6)\n","Installing collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed opencv-python-4.5.5.62\n","Collecting opencv-contrib-python==4.5.5.62\n","  Downloading opencv_contrib_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (66.6 MB)\n","\u001b[K     |████████████████████████████████| 66.6 MB 132 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.5.5.62) (1.21.6)\n","Installing collected packages: opencv-contrib-python\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-4.5.5.62\n"]}]},{"cell_type":"code","source":["import os\n","import imageio\n","import numpy as np\n","from disk import DISK, Features\n","import torch\n","import torch.nn.functional as TorchFunctional\n","from torch.utils.data import DataLoader\n","from torch_dimcheck import dimchecked\n","from tqdm import tqdm\n","import argparse\n","from functools import partial\n","import h5py\n","import matplotlib.pyplot as plt\n","from glob import glob\n","import csv\n","import cv2\n","\n","if not torch.cuda.is_available():\n","    print('You may want to enable the GPU switch?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edzbXRMd8XuH","executionInfo":{"status":"ok","timestamp":1652270379057,"user_tz":-180,"elapsed":3036,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"6d224ddf-8143-4146-9424-00d70223e9fb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["You may want to enable the GPU switch?\n"]}]},{"cell_type":"code","source":["dry_run = True"],"metadata":{"id":"I5F7In2l9Ls1","executionInfo":{"status":"ok","timestamp":1652270379058,"user_tz":-180,"elapsed":6,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/gdrive/MyDrive/imc/superglue-pretrained-network\")\n","\n","from models.matching import SuperGlue, Matching\n","from models.utils import (compute_pose_error, compute_epipolar_error,\n","                          estimate_pose, make_matching_plot,\n","                          error_colormap, AverageTimer, pose_auc, read_image,\n","                          rotate_intrinsics, rotate_pose_inplane,\n","                          scale_intrinsics)"],"metadata":{"id":"X9pbbMrjqGgI","executionInfo":{"status":"ok","timestamp":1652270383424,"user_tz":-180,"elapsed":4371,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Utility"],"metadata":{"id":"MgTgZce98nev"}},{"cell_type":"code","source":["def FlattenMatrix(M, num_digits=8):\n","    '''Convenience function to write CSV files.'''\n","    \n","    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n","\n","\n","def BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n","    '''Convenience function to stack two images with different sizes.'''\n","    \n","    if background != 0 and background != 1:\n","        background = 1\n","    if axis != 0 and axis != 1:\n","        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n","\n","    h1, w1, _ = im1.shape\n","    h2, w2, _ = im2.shape\n","\n","    if axis == 1:\n","        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n","        if h1 > h2:\n","            voff1, voff2 = 0, (h1 - h2) // 2\n","        else:\n","            voff1, voff2 = (h2 - h1) // 2, 0\n","        hoff1, hoff2 = 0, w1 + margin\n","    else:\n","        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n","        if w1 > w2:\n","            hoff1, hoff2 = 0, (w1 - w2) // 2\n","        else:\n","            hoff1, hoff2 = (w2 - w1) // 2, 0\n","        voff1, voff2 = 0, h1 + margin\n","    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n","    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n","\n","    return (composite, (voff1, voff2), (hoff1, hoff2))\n","\n","\n","def DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n","    '''Draw keypoints and matches.'''\n","    \n","    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n","\n","    # Draw all keypoints.\n","    for coord_a, coord_b in zip(kp1, kp2):\n","        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n","        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n","    \n","    # Draw matches, and highlight keypoints used in matches.\n","    for idx_a, idx_b in matches:\n","        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n","        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n","        composite = cv2.line(composite,\n","                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n","                                   int(kp1[idx_a][1] + v_offset[0])]),\n","                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n","                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n","    \n","    return composite"],"metadata":{"id":"tPVk8H0-8e-o","executionInfo":{"status":"ok","timestamp":1652270383424,"user_tz":-180,"elapsed":5,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Data reading"],"metadata":{"id":"WyC8E4MpIMZV"}},{"cell_type":"code","source":["# Read the pairs file.\n","\n","src = '/content/gdrive/MyDrive/imc/input'\n","\n","test_samples = []\n","with open(f'{src}/test.csv') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    for i, row in enumerate(reader):\n","        # Skip header.\n","        if i == 0:\n","            continue\n","        test_samples += [row]\n","\n","if dry_run:\n","    for sample in test_samples:\n","        print(sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVBj_pLt83HG","executionInfo":{"status":"ok","timestamp":1652270383747,"user_tz":-180,"elapsed":327,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"6f04fb0c-4265-4972-8e78-a06f2f9c30fc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['googleurban;1cf87530;a5a9975574c94ff9a285f58c39b53d2c-0143f47ee9e54243a1b8454f3e91621a', '1cf87530', 'a5a9975574c94ff9a285f58c39b53d2c', '0143f47ee9e54243a1b8454f3e91621a']\n","['googleurban;6ceaefff;39563e58b2b7411da3f06427c9ee4239-0303b05ca0cb46959eac430e4b2472ca', '6ceaefff', '39563e58b2b7411da3f06427c9ee4239', '0303b05ca0cb46959eac430e4b2472ca']\n","['googleurban;d91db836;81dd07fb7b9a4e01996cee637f91ca1a-0006b1337a0347f49b4e651c035dfa0e', 'd91db836', '81dd07fb7b9a4e01996cee637f91ca1a', '0006b1337a0347f49b4e651c035dfa0e']\n"]}]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"AfNbPfKjIWer"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    '--height', default=None, type=int,\n","    help='rescaled height (px). If unspecified, image is not resized in height dimension'\n",")\n","parser.add_argument(\n","    '--width', default=None, type=int,\n","    help='rescaled width (px). If unspecified, image is not resized in width dimension'\n",")\n","parser.add_argument(\n","    '--image-extension', default='jpg', type=str,\n","    help='This script ill process all files which match `image-path/*.{--image-extension}`'\n",")\n","parser.add_argument(\n","    '--f16', action='store_true',\n","    help='Store descriptors in fp16 (half precision) format'\n",")\n","parser.add_argument('--window', type=int, default=5, help='NMS window size')\n","parser.add_argument(\n","    '--n', type=int, default=None,\n","    help='Maximum number of features to extract. If unspecified, the number is not limited'\n",")\n","parser.add_argument(\n","    '--desc-dim', type=int, default=256,\n","    help='descriptor dimension. Needs to match the checkpoint value.'\n",")\n","parser.add_argument(\n","    '--mode', choices=['nms', 'rng'], default='nms',\n","    help=('Whether to extract features using the non-maxima suppresion mode or '\n","          'through training-time grid sampling technique')\n",")\n","parser.add_argument(\n","     '--model_path', type=str, default='/content/pretrained/disk-depth.pth',\n","    help=\"Path to the model's .pth save file\"\n",")\n","\n","parser.add_argument('--detection-scores', action='store_true')\n","\n","\n","# Hacky copy-paste: parameters go here.\n","args = parser.parse_args('--n 2048 --window 9 --desc-dim 256 --height 768 --width 768 --image-extension png --detection-scores'.split())\n","print(args)\n","\n","DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","CPU = torch.device('cpu')\n","state_dict = torch.load(args.model_path, map_location='cpu')\n","\n","# For compatibility with older model saves.\n","if 'extractor' in state_dict:\n","    weights = state_dict['extractor']\n","elif 'disk' in state_dict:\n","    weights = state_dict['disk']\n","else:\n","    raise KeyError('Incompatible weight file!')\n","\n","model = DISK(window=args.window, desc_dim=128)\n","model.load_state_dict(weights)\n","model = model.to(DEV)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytwRxds_9wxr","executionInfo":{"status":"ok","timestamp":1652274598898,"user_tz":-180,"elapsed":5,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"9f178af1-5e2e-4cdd-df82-988edd6c2090"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(desc_dim=256, detection_scores=True, f16=False, height=768, image_extension='png', mode='nms', model_path='/content/pretrained/disk-depth.pth', n=2048, width=768, window=9)\n"]}]},{"cell_type":"markdown","source":["# Feature extraction"],"metadata":{"id":"nYFHS_7fIfX1"}},{"cell_type":"code","source":["!rm -rf features\n","\n","class Image:\n","    def __init__(self, bitmap: ['C', 'H', 'W'], fname: str, orig_shape=None):\n","        self.bitmap     = bitmap\n","        self.fname      = fname\n","        if orig_shape is None:\n","            self.orig_shape = self.bitmap.shape[1:]\n","        else:\n","            self.orig_shape = orig_shape\n","\n","    def resize_to(self, shape):\n","        return Image(\n","            self._pad(self._interpolate(self.bitmap, shape), shape),\n","            self.fname,\n","            orig_shape=self.bitmap.shape[1:],\n","        )\n","\n","    @dimchecked\n","    def to_image_coord(self, xys: [2, 'N']) -> ([2, 'N'], ['N']):\n","        f, _size = self._compute_interpolation_size(self.bitmap.shape[1:])\n","        scaled = xys / f\n","\n","        h, w = self.orig_shape\n","        x, y = scaled\n","\n","        mask = (0 <= x) & (x < w) & (0 <= y) & (y < h)\n","\n","        return scaled, mask\n","\n","    def _compute_interpolation_size(self, shape):\n","        x_factor = self.orig_shape[0] / shape[0]\n","        y_factor = self.orig_shape[1] / shape[1]\n","\n","        f = 1 / max(x_factor, y_factor)\n","\n","        if x_factor > y_factor:\n","            new_size = (shape[0], int(f * self.orig_shape[1]))\n","        else:\n","            new_size = (int(f * self.orig_shape[0]), shape[1])\n","\n","        return f, new_size\n","\n","    @dimchecked\n","    def _interpolate(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n","        _f, size = self._compute_interpolation_size(shape)\n","        return TorchFunctional.interpolate(\n","            image.unsqueeze(0),\n","            size=size,\n","            mode='bilinear',\n","            align_corners=False,\n","        ).squeeze(0)\n","\n","    @dimchecked\n","    def _pad(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n","        x_pad = shape[0] - image.shape[1]\n","        y_pad = shape[1] - image.shape[2]\n","\n","        if x_pad < 0 or y_pad < 0:\n","            raise ValueError(\"Attempting to pad by negative value\")\n","\n","        return TorchFunctional.pad(image, (0, y_pad, 0, x_pad))\n","\n","    \n","class SceneDataset:\n","    def __init__(self, image_path, crop_size=(None, None)):\n","        self.image_path = image_path\n","        self.crop_size  = crop_size\n","        self.names = [p for p in os.listdir(image_path) \\\n","                      if p.endswith(args.image_extension)]\n","\n","    def __len__(self):\n","        return len(self.names)\n","\n","    def __getitem__(self, ix):\n","        name   = self.names[ix]\n","        path   = os.path.join(self.image_path, name) \n","        img    = np.ascontiguousarray(imageio.imread(path))\n","        tensor = torch.from_numpy(img).to(torch.float32)\n","\n","        if len(tensor.shape) == 2: # some images may be grayscale\n","            tensor = tensor.unsqueeze(-1).expand(-1, -1, 3)\n","\n","        bitmap              = tensor.permute(2, 0, 1) / 255.\n","        extensionless_fname = os.path.splitext(name)[0]\n","        image = Image(bitmap, extensionless_fname)\n","\n","        if self.crop_size != (None, None):\n","            image = image.resize_to(self.crop_size)\n","\n","        return image\n","\n","    @staticmethod\n","    def collate_fn(images):\n","        bitmaps = torch.stack([im.bitmap for im in images], dim=0)\n","        \n","        return bitmaps, images\n","\n","\n","def extract(dataset, save_path):\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=1,\n","        pin_memory=True,\n","        collate_fn=dataset.collate_fn,\n","        num_workers=4,\n","    )\n","\n","    if args.mode == 'nms':\n","        extract = partial(\n","            model.features,\n","            kind='nms',\n","            window_size=args.window,\n","            cutoff=0.,\n","            n=args.n\n","        )\n","    else:\n","        extract = partial(model.features, kind='rng')\n","\n","    os.makedirs(os.path.join(save_path), exist_ok=True)\n","    keypoint_h5   = h5py.File(os.path.join(save_path, 'keypoints.h5'), 'w')\n","    descriptor_h5 = h5py.File(os.path.join(save_path, 'descriptors.h5'), 'w')\n","    if args.detection_scores:\n","        score_h5 = h5py.File(os.path.join(save_path, 'scores.h5'), 'w')\n","\n","    pbar = tqdm(dataloader)\n","    for bitmaps, images in pbar:\n","        bitmaps = bitmaps.to(DEV, non_blocking=True)\n","\n","        with torch.no_grad():\n","            try:\n","                batched_features = extract(bitmaps)\n","            except RuntimeError as e:\n","                if 'U-Net failed' in str(e):\n","                    msg = ('Please use input size which is multiple of 16 (or '\n","                           'adjust the --height and --width flags to let this '\n","                           'script rescale it automatically). This is because '\n","                           'we internally use a U-Net with 4 downsampling '\n","                           'steps, each by a factor of 2, therefore 2^4=16.')\n","\n","                    raise RuntimeError(msg) from e\n","                else:\n","                    raise\n","\n","        for features, image in zip(batched_features.flat, images):\n","              features = features.to(CPU)\n","\n","              kps_crop_space = features.kp.T\n","              kps_img_space, mask = image.to_image_coord(kps_crop_space)\n","\n","              keypoints   = kps_img_space.numpy().T[mask]\n","              descriptors = features.desc.numpy()[mask]\n","              scores      = features.kp_logp.numpy()[mask]\n","\n","              order = np.argsort(scores)[::-1]\n","\n","              keypoints   = keypoints[order]\n","              descriptors = descriptors[order].T\n","              scores      = scores[order]\n","\n","              print(descriptors.shape)\n","\n","              assert descriptors.shape[0] == args.desc_dim\n","              assert keypoints.shape[1] == 2\n","\n","              if args.f16:\n","                  descriptors = descriptors.astype(np.float16)\n","\n","              keypoint_h5.create_dataset(image.fname, data=keypoints)\n","              descriptor_h5.create_dataset(image.fname, data=descriptors)\n","\n","              if args.detection_scores:\n","                  score_h5.create_dataset(image.fname, data=scores)\n","\n","              pbar.set_postfix(n=keypoints.shape[0])"],"metadata":{"id":"xibNz1lTBMz4","executionInfo":{"status":"ok","timestamp":1652274343125,"user_tz":-180,"elapsed":2393,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# Extract features for every image in every folder.\n","for dataset_folder in glob('/content/gdrive/MyDrive/imc/input/test_images/*'):\n","    batch_id = dataset_folder.split('/')[-1]\n","    print(f'Processing \"{dataset_folder}\"')\n","    dataset = SceneDataset(dataset_folder, crop_size=(args.height, args.width))\n","    described_samples = extract(dataset, f'features/{batch_id}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-g9TL7UBOZ2","executionInfo":{"status":"ok","timestamp":1652274415221,"user_tz":-180,"elapsed":69602,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"82d1fbc8-36f2-4d1e-ea33-01ad8d2cb061"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Processing \"/content/gdrive/MyDrive/imc/input/test_images/1cf87530\"\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:749: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n","  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"," 50%|█████     | 1/2 [00:12<00:12, 12.63s/it, n=1186]"]},{"output_type":"stream","name":"stdout","text":["(256, 1186)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:24<00:00, 12.16s/it, n=1107]\n"]},{"output_type":"stream","name":"stdout","text":["(256, 1107)\n","Processing \"/content/gdrive/MyDrive/imc/input/test_images/6ceaefff\"\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 1/2 [00:13<00:13, 13.66s/it, n=674]"]},{"output_type":"stream","name":"stdout","text":["(256, 674)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:23<00:00, 11.73s/it, n=742]\n"]},{"output_type":"stream","name":"stdout","text":["(256, 742)\n","Processing \"/content/gdrive/MyDrive/imc/input/test_images/d91db836\"\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 1/2 [00:10<00:10, 10.85s/it, n=838]"]},{"output_type":"stream","name":"stdout","text":["(256, 838)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:20<00:00, 10.04s/it, n=777]"]},{"output_type":"stream","name":"stdout","text":["(256, 777)\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 2/2 [00:20<00:00, 10.26s/it, n=777]\n"]}]},{"cell_type":"markdown","source":["# Matching"],"metadata":{"id":"Gk_wS2WqNV8a"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","resize = [-1, ]\n","resize_float = True\n","\n","config = {\n","    \"superglue\": {\n","        \"weights\": \"outdoor\",\n","        \"sinkhorn_iterations\": 20,\n","        \"match_threshold\": 0.2,\n","        \"descriptor_dim\" : 256\n","    }\n","}\n","matching = Matching(config).eval().to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTtuHTHeOs6j","executionInfo":{"status":"ok","timestamp":1652274461310,"user_tz":-180,"elapsed":895,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"6be09849-a193-4006-a249-eb3c43e75755"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded SuperPoint model\n","Loaded SuperGlue model (\"outdoor\" weights)\n"]}]},{"cell_type":"code","source":["def totensor(frame, device):\n","    return torch.from_numpy(frame).float()[None, None].to(device)\n","\n","def score2tensor(frame, device):\n","    return torch.from_numpy(frame)[None].float().to(device)\n","\n","def kpts2tensor(frame, device):\n","    return torch.from_numpy(frame).float().to(device)"],"metadata":{"id":"XPdt--ixGyK1","executionInfo":{"status":"ok","timestamp":1652274462319,"user_tz":-180,"elapsed":2,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import gc\n","\n","how_many_to_fill = -1\n","\n","F_dict = {}\n","for i, row in enumerate(test_samples):\n","    sample_id, batch_id, image_1_id, image_2_id = row\n","\n","    with h5py.File(f'features/{batch_id}/keypoints.h5', 'r') as kp_dict, h5py.File(f'features/{batch_id}/descriptors.h5', 'r') as desc_dict:\n","\n","        desc0, desc1 = frame2tensor(desc_dict[image_1_id][()], device), frame2tensor(desc_dict[image_2_id][()], device)\n","        kpts0, kpts1 = frame2tensor(kp_dict[image_1_id][()], device), frame2tensor(kp_dict[image_1_id][()], device)\n","\n","        pred = matching({\"image0\": desc0, \"image1\": desc1})\n","        pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n","        kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n","        matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n","\n","        valid = matches > -1\n","        mkpts1 = kpts1[valid]\n","        mkpts2 = kpts2[matches[valid]]\n","        mconf = conf[valid]\n","\n","      \n","        if len(mkpts1) > 8:\n","            F, inliers = cv2.findFundamentalMat(mkpts1, \n","                                                mkpts2, \n","                                                cv2.USAC_MAGSAC,\n","                                                ransacReprojThreshold=0.75,\n","                                                confidence=0.99,\n","                                                maxIters=10000)\n","        \n","            inliers = inliers > 0\n","            assert F.shape == (3, 3), 'Malformed F?'\n","            F_dict[sample_id] = F\n","        else:\n","            F_dict[sample_id] = np.zeros((3, 3))\n","            continue\n","        gc.collect()\n","\n","        if dry_run:\n","            image_1 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_1_id}.png'), cv2.COLOR_BGR2RGB)\n","            image_2 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_2_id}.png'), cv2.COLOR_BGR2RGB)\n","            matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inliers) if is_inlier])\n","\n","            im_inliers = DrawMatches(image_1, image_2, kp_dict[image_1_id][()], kp_dict[image_2_id][()], matches_after_ransac)\n","            fig = plt.figure(figsize=(15, 15))\n","            plt.title(f'{image_1_id}-{image_2_id}')\n","            plt.imshow(im_inliers)\n","            plt.axis('off')\n","            plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"LCVgIfL8P6_6","executionInfo":{"status":"error","timestamp":1652272704193,"user_tz":-180,"elapsed":1010,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"1aaa75ef-952b-4e54-a3d6-9bcb9bc9d61d"},"execution_count":51,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-5c8578641c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'features/{batch_id}/keypoints.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'features/{batch_id}/descriptors.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdesc_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdesc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_2_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mkpts0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpts1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'frame2tensor' is not defined"]}]},{"cell_type":"code","source":["with h5py.File(f'features/{batch_id}/keypoints.h5', 'r') as kp_dict, h5py.File(f'features/{batch_id}/descriptors.h5', 'r') as desc_dict, h5py.File(f'features/{batch_id}/scores.h5', 'r') as scores_dict:\n","\n","        desc0, desc1 = totensor(desc_dict[image_1_id][()], device), totensor(desc_dict[image_2_id][()], device)\n","        kpts0, kpts1 = kpts2tensor(kp_dict[image_1_id][()], device), totensor(kp_dict[image_2_id][()], device)\n","        scores0, scores1 = score2tensor(scores_dict[image_1_id][()], device), score2tensor(scores_dict[image_2_id][()], device)\n","        \n","        pred = matching({\"keypoints0\": kpts0, \"keypoints1\": kpts1, \"descriptors0\": desc0, \"descriptors1\": desc1, 'image0': desc0, 'image1': desc1, 'scores0': scores0, 'scores1': scores1})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"id":"9ukb4iZYM348","executionInfo":{"status":"error","timestamp":1652274470524,"user_tz":-180,"elapsed":454,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"5beab1ce-2ecf-4ea0-f2b5-eedf8e148e8a"},"execution_count":85,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-85-264067a4435e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'features/{batch_id}/keypoints.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'features/{batch_id}/descriptors.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdesc_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'features/{batch_id}/scores.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscores_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdesc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_2_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mkpts0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkpts1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkpts2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_2_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mscores0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_1_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_2_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'a5a9975574c94ff9a285f58c39b53d2c' doesn't exist)\""]}]},{"cell_type":"code","source":["print(desc0.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFg8c4A_UX8v","executionInfo":{"status":"ok","timestamp":1652273570462,"user_tz":-180,"elapsed":350,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"a4bf6084-02e4-4309-de26-7914a7876caf"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 128, 2048])\n"]}]},{"cell_type":"code","source":["print(kpts0.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcFG7oZSbobi","executionInfo":{"status":"ok","timestamp":1652273570816,"user_tz":-180,"elapsed":4,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"3baf2924-fd99-4a43-afb3-a9794b8d4aa0"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2048, 2])\n"]}]},{"cell_type":"code","source":["print(scores0.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqE2m9BjbrUb","executionInfo":{"status":"ok","timestamp":1652273571217,"user_tz":-180,"elapsed":3,"user":{"displayName":"Vadim Zubkov","userId":"04300401414036288085"}},"outputId":"4177e362-cbe1-4bea-b225-50f41f120835"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2048])\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"gwsEC5ThbwxK"},"execution_count":null,"outputs":[]}]}